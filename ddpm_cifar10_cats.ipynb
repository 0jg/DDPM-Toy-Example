{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0jg/DDPM-Toy-Example/blob/main/ddpm_cifar10_cats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzFBDZG3ZZ2P"
      },
      "source": [
        "# DDPM Diffusion Models: CIFAR-10 Cats Example\n",
        "\n",
        "This notebook demonstrates Denoising Diffusion Probabilistic Models (DDPM) applied to real images \u2014 specifically cat images from the CIFAR-10 dataset. Building on the concepts from the toy example, we'll see how these same principles scale to generate 32\u00d732 RGB images.\n",
        "\n",
        "## What We'll Cover\n",
        "\n",
        "We'll implement the forward diffusion process for images, then train a U-Net style neural network to reverse this process. The architecture is more sophisticated than the simple MLP used for 2D points, but the core mathematics remain identical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-U-UaMNZZ2T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('mps' if torch.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Plotting utils\n",
        "plt.rcParams.update({\n",
        "    'font.family': 'serif',\n",
        "    'font.serif': ['STIXGeneral', 'DejaVu Serif'],\n",
        "    'mathtext.fontset': 'stix',\n",
        "    'font.size': 12,\n",
        "    'figure.dpi': 150\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-eCFy4QZZ2U"
      },
      "source": [
        "## Step 1: Load CIFAR-10 Cat Images\n",
        "\n",
        "CIFAR-10 contains 60,000 32\u00d732 colour images in 10 classes. Class 3 is \"cat\", and we'll extract all cat images for our training set. We normalize pixel values to [-1, 1] which is standard for diffusion models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataseT-ZZ2U"
      },
      "outputs": [],
      "source": [
        "def load_cifar10_cats():\n",
        "    \"\"\"\n",
        "    Load cat images from CIFAR-10.\n",
        "    \n",
        "    CIFAR-10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
        "    Cat is class index 3.\n",
        "    \n",
        "    Returns:\n",
        "        Dataset of cat images normalized to [-1, 1]\n",
        "    \"\"\"\n",
        "    # Transform: convert to tensor and normalize to [-1, 1]\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Maps [0,1] to [-1,1]\n",
        "    ])\n",
        "    \n",
        "    # Download and load training data\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "    \n",
        "    # Filter for cats (class 3)\n",
        "    cat_class = 3\n",
        "    train_cat_indices = [i for i, (_, label) in enumerate(train_dataset) if label == cat_class]\n",
        "    test_cat_indices = [i for i, (_, label) in enumerate(test_dataset) if label == cat_class]\n",
        "    \n",
        "    train_cats = Subset(train_dataset, train_cat_indices)\n",
        "    test_cats = Subset(test_dataset, test_cat_indices)\n",
        "    \n",
        "    print(f\"Found {len(train_cats)} cat images in training set\")\n",
        "    print(f\"Found {len(test_cats)} cat images in test set\")\n",
        "    print(f\"Image shape: 3 x 32 x 32 (RGB)\")\n",
        "    \n",
        "    return train_cats, test_cats\n",
        "\n",
        "# Load the dataset\n",
        "train_cats, test_cats = load_cifar10_cats()\n",
        "\n",
        "# Display some cat images\n",
        "fig, axes = plt.subplots(2, 8, figsize=(12, 3))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    img, _ = train_cats[i]\n",
        "    # Convert from [-1,1] to [0,1] for display\n",
        "    img = (img.permute(1, 2, 0) + 1) / 2\n",
        "    ax.imshow(img.clip(0, 1))\n",
        "    ax.axis('off')\n",
        "plt.suptitle('Sample Cat Images from CIFAR-10', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scheduleZZ2U"
      },
      "source": [
        "## Step 2: Define the Forward Diffusion Process\n",
        "\n",
        "The forward process is identical to the toy example \u2014 we gradually add Gaussian noise over T timesteps. The only difference is that our data is now 3\u00d732\u00d732 images instead of 2D points. The mathematics remain exactly the same:\n",
        "\n",
        "$$q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "schedule-codeZZ2U"
      },
      "outputs": [],
      "source": [
        "class DDPMSchedule:\n",
        "    \"\"\"\n",
        "    Manages the noise schedule for the diffusion process.\n",
        "    \n",
        "    Same as the toy example, with parameters tuned for images.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
        "        self.num_timesteps = num_timesteps\n",
        "        \n",
        "        # Linear beta schedule\n",
        "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
        "        \n",
        "        # Pre-compute useful quantities\n",
        "        self.alphas = 1.0 - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
        "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "        \n",
        "        # For q(x_t | x_0)\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
        "        \n",
        "        # For posterior q(x_{t-1} | x_t, x_0)\n",
        "        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)\n",
        "        self.posterior_variance = self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
        "        \n",
        "    def to(self, device):\n",
        "        \"\"\"Move all tensors to specified device.\"\"\"\n",
        "        self.betas = self.betas.to(device)\n",
        "        self.alphas = self.alphas.to(device)\n",
        "        self.alphas_cumprod = self.alphas_cumprod.to(device)\n",
        "        self.alphas_cumprod_prev = self.alphas_cumprod_prev.to(device)\n",
        "        self.sqrt_alphas_cumprod = self.sqrt_alphas_cumprod.to(device)\n",
        "        self.sqrt_one_minus_alphas_cumprod = self.sqrt_one_minus_alphas_cumprod.to(device)\n",
        "        self.sqrt_recip_alphas = self.sqrt_recip_alphas.to(device)\n",
        "        self.posterior_variance = self.posterior_variance.to(device)\n",
        "        return self\n",
        "    \n",
        "    def q_sample(self, x_0, t, noise=None):\n",
        "        \"\"\"\n",
        "        Sample from q(x_t | x_0) - the forward diffusion process.\n",
        "        \"\"\"\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_0)\n",
        "        \n",
        "        sqrt_alpha_cumprod_t = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
        "        sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
        "        \n",
        "        return sqrt_alpha_cumprod_t * x_0 + sqrt_one_minus_alpha_cumprod_t * noise\n",
        "\n",
        "# Create schedule\n",
        "schedule = DDPMSchedule(num_timesteps=1000).to(device)\n",
        "print(f\"Created noise schedule with {schedule.num_timesteps} timesteps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vis-forwardZZ2U"
      },
      "source": [
        "## Visualize the Forward Diffusion Process\n",
        "\n",
        "Let's see what happens to a cat image as we progressively add noise. Watch how the recognizable cat gradually dissolves into pure Gaussian noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vis-forward-codeZZ2U"
      },
      "outputs": [],
      "source": [
        "# Get a sample cat image\n",
        "sample_img, _ = train_cats[0]\n",
        "x_0 = sample_img.unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "# Visualize diffusion at different timesteps\n",
        "timesteps_to_show = [0, 100, 250, 500, 750, 999]\n",
        "fig, axes = plt.subplots(1, len(timesteps_to_show), figsize=(12, 2.5))\n",
        "\n",
        "for ax, t in zip(axes, timesteps_to_show):\n",
        "    if t == 0:\n",
        "        x_t = x_0\n",
        "    else:\n",
        "        t_tensor = torch.tensor([t], device=device)\n",
        "        x_t = schedule.q_sample(x_0, t_tensor)\n",
        "    \n",
        "    # Convert to displayable format\n",
        "    img = x_t[0].cpu().permute(1, 2, 0)\n",
        "    img = (img + 1) / 2  # [-1,1] to [0,1]\n",
        "    ax.imshow(img.clip(0, 1))\n",
        "    ax.set_title(f't = {t}')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Forward Diffusion Process: Cat \u2192 Noise', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"As t increases, the cat image gradually becomes indistinguishable from random noise.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unet-introZZ2U"
      },
      "source": [
        "## Step 3: Define the U-Net Denoising Network\n",
        "\n",
        "For images, we need a more sophisticated architecture than the simple MLP used for 2D points. The U-Net architecture is the standard choice for diffusion models because:\n",
        "\n",
        "1. **Encoder-Decoder structure**: Captures both local details and global context\n",
        "2. **Skip connections**: Preserve fine details during upsampling\n",
        "3. **Multi-scale processing**: Efficiently handles spatial hierarchies in images\n",
        "\n",
        "The network still predicts the noise $\\varepsilon$ that was added, conditioned on the timestep $t$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unet-codeZZ2U"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Embed the timestep using sinusoidal functions.\n",
        "    Same as the toy example - allows the network to understand \"how noisy\" the input is.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "    \n",
        "    def forward(self, t):\n",
        "        device = t.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = np.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = t[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual block with time embedding injection.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        self.time_mlp = nn.Linear(time_emb_dim, out_channels)\n",
        "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
        "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
        "        \n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels, 1)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "    \n",
        "    def forward(self, x, t_emb):\n",
        "        h = self.conv1(x)\n",
        "        h = self.norm1(h)\n",
        "        h = F.silu(h)\n",
        "        \n",
        "        # Add time embedding\n",
        "        t_emb = self.time_mlp(t_emb)[:, :, None, None]\n",
        "        h = h + t_emb\n",
        "        \n",
        "        h = self.conv2(h)\n",
        "        h = self.norm2(h)\n",
        "        h = F.silu(h)\n",
        "        \n",
        "        return h + self.shortcut(x)\n",
        "\n",
        "\n",
        "class SimpleUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified U-Net for 32x32 images.\n",
        "    \n",
        "    Architecture:\n",
        "    - Encoder: 32x32 \u2192 16x16 \u2192 8x8\n",
        "    - Bottleneck: 8x8\n",
        "    - Decoder: 8x8 \u2192 16x16 \u2192 32x32\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, out_channels=3, time_emb_dim=128):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Time embedding\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
        "            nn.Linear(time_emb_dim, time_emb_dim),\n",
        "            nn.SiLU(),\n",
        "        )\n",
        "        \n",
        "        # Encoder\n",
        "        self.enc1 = ResidualBlock(in_channels, 64, time_emb_dim)\n",
        "        self.enc2 = ResidualBlock(64, 128, time_emb_dim)\n",
        "        self.enc3 = ResidualBlock(128, 256, time_emb_dim)\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        \n",
        "        # Bottleneck\n",
        "        self.bottleneck = ResidualBlock(256, 256, time_emb_dim)\n",
        "        \n",
        "        # Decoder\n",
        "        self.up3 = nn.ConvTranspose2d(256, 256, 2, stride=2)\n",
        "        self.dec3 = ResidualBlock(512, 128, time_emb_dim)  # 256 + 256 from skip\n",
        "        \n",
        "        self.up2 = nn.ConvTranspose2d(128, 128, 2, stride=2)\n",
        "        self.dec2 = ResidualBlock(256, 64, time_emb_dim)   # 128 + 128 from skip\n",
        "        \n",
        "        self.up1 = nn.ConvTranspose2d(64, 64, 2, stride=2)\n",
        "        self.dec1 = ResidualBlock(128, 64, time_emb_dim)   # 64 + 64 from skip\n",
        "        \n",
        "        # Output\n",
        "        self.out = nn.Conv2d(64, out_channels, 1)\n",
        "    \n",
        "    def forward(self, x, t):\n",
        "        # Time embedding\n",
        "        t_emb = self.time_mlp(t)\n",
        "        \n",
        "        # Encoder path\n",
        "        e1 = self.enc1(x, t_emb)           # 32x32\n",
        "        e2 = self.enc2(self.pool(e1), t_emb)  # 16x16\n",
        "        e3 = self.enc3(self.pool(e2), t_emb)  # 8x8\n",
        "        \n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(self.pool(e3), t_emb)  # 4x4\n",
        "        \n",
        "        # Decoder path with skip connections\n",
        "        d3 = self.up3(b)                          # 8x8\n",
        "        d3 = self.dec3(torch.cat([d3, e3], dim=1), t_emb)\n",
        "        \n",
        "        d2 = self.up2(d3)                         # 16x16\n",
        "        d2 = self.dec2(torch.cat([d2, e2], dim=1), t_emb)\n",
        "        \n",
        "        d1 = self.up1(d2)                         # 32x32\n",
        "        d1 = self.dec1(torch.cat([d1, e1], dim=1), t_emb)\n",
        "        \n",
        "        return self.out(d1)\n",
        "\n",
        "\n",
        "# Create model\n",
        "model = SimpleUNet().to(device)\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Created U-Net with {num_params:,} parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trainingZZ2U"
      },
      "source": [
        "## Step 4: Training the Denoising Network\n",
        "\n",
        "The training process is identical to the toy example:\n",
        "\n",
        "1. Take a batch of images $x_0$\n",
        "2. Sample random timesteps $t$\n",
        "3. Sample noise $\\varepsilon$ and create noisy images $x_t$\n",
        "4. Predict the noise with our U-Net\n",
        "5. Minimize MSE between predicted and actual noise\n",
        "\n",
        "$$\\mathcal{L} = \\mathbb{E}_{x_0, \\varepsilon, t}\\left[\\|\\varepsilon - \\varepsilon_\\theta(x_t, t)\\|^2\\right]$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training-codeZZ2U"
      },
      "outputs": [],
      "source": [
        "def train_ddpm(model, dataset, schedule, num_epochs=50, batch_size=64, lr=1e-3):\n",
        "    \"\"\"\n",
        "    Train the denoising network on images.\n",
        "    \"\"\"\n",
        "    # Prepare data loader - extract just images (not labels)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "    \n",
        "    model.train()\n",
        "    losses = []\n",
        "    \n",
        "    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
        "        epoch_loss = 0.0\n",
        "        num_batches = 0\n",
        "        \n",
        "        for batch_data in dataloader:\n",
        "            # Handle both (image, label) tuples and just images\n",
        "            if isinstance(batch_data, (list, tuple)):\n",
        "                x_0 = batch_data[0].to(device)\n",
        "            else:\n",
        "                x_0 = batch_data.to(device)\n",
        "            \n",
        "            batch_size_actual = x_0.shape[0]\n",
        "            \n",
        "            # Sample random timesteps\n",
        "            t = torch.randint(0, schedule.num_timesteps, (batch_size_actual,), device=device)\n",
        "            \n",
        "            # Sample noise\n",
        "            noise = torch.randn_like(x_0)\n",
        "            \n",
        "            # Create noisy images\n",
        "            x_t = schedule.q_sample(x_0, t, noise)\n",
        "            \n",
        "            # Predict noise\n",
        "            predicted_noise = model(x_t, t.float())\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = criterion(predicted_noise, noise)\n",
        "            \n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            num_batches += 1\n",
        "        \n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        losses.append(avg_loss)\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
        "    \n",
        "    return losses\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "print(\"Note: For best results, train for 100+ epochs. Using 50 for demonstration.\")\n",
        "losses = train_ddpm(model, train_cats, schedule, num_epochs=50, batch_size=64, lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot-lossZZ2U"
      },
      "outputs": [],
      "source": [
        "# Plot training loss\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(losses, color='k')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nTraining complete! Final loss: {losses[-1]:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "samplingZZ2U"
      },
      "source": [
        "## Step 5: Sampling from the Learned Model\n",
        "\n",
        "Now we generate new cat images! The sampling process is the same as the toy example:\n",
        "\n",
        "1. Start with pure Gaussian noise\n",
        "2. For each timestep from T-1 to 0, use the model to predict and remove noise\n",
        "3. The final result should be a new cat image!\n",
        "\n",
        "$$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\varepsilon_\\theta(x_t, t)\\right) + \\sigma_t z$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sampling-codeZZ2U"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample_ddpm(model, schedule, num_samples=16, image_size=32, channels=3, save_trajectory=False):\n",
        "    \"\"\"\n",
        "    Generate images by reversing the diffusion process.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Start from pure noise\n",
        "    x = torch.randn(num_samples, channels, image_size, image_size, device=device)\n",
        "    \n",
        "    trajectory = [x.cpu()] if save_trajectory else None\n",
        "    \n",
        "    # Reverse diffusion\n",
        "    for t in tqdm(reversed(range(schedule.num_timesteps)), desc=\"Sampling\", total=schedule.num_timesteps):\n",
        "        t_batch = torch.full((num_samples,), t, device=device, dtype=torch.float)\n",
        "        \n",
        "        # Predict noise\n",
        "        predicted_noise = model(x, t_batch)\n",
        "        \n",
        "        # Compute x_{t-1}\n",
        "        alpha_t = schedule.alphas[t]\n",
        "        alpha_cumprod_t = schedule.alphas_cumprod[t]\n",
        "        beta_t = schedule.betas[t]\n",
        "        \n",
        "        # Mean of p(x_{t-1} | x_t)\n",
        "        mean = (1 / torch.sqrt(alpha_t)) * (\n",
        "            x - (beta_t / torch.sqrt(1 - alpha_cumprod_t)) * predicted_noise\n",
        "        )\n",
        "        \n",
        "        # Add noise (except for t=0)\n",
        "        if t > 0:\n",
        "            noise = torch.randn_like(x)\n",
        "            sigma_t = torch.sqrt(schedule.posterior_variance[t])\n",
        "            x = mean + sigma_t * noise\n",
        "        else:\n",
        "            x = mean\n",
        "        \n",
        "        # Save trajectory at specific steps\n",
        "        if save_trajectory and t in [999, 900, 750, 500, 250, 100, 50, 0]:\n",
        "            trajectory.append(x.cpu())\n",
        "    \n",
        "    if save_trajectory:\n",
        "        return x, trajectory\n",
        "    return x\n",
        "\n",
        "# Generate samples\n",
        "print(\"Generating cat images...\")\n",
        "generated_images, trajectory = sample_ddpm(model, schedule, num_samples=16, save_trajectory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vis-reverseZZ2U"
      },
      "source": [
        "## Visualize the Reverse Process\n",
        "\n",
        "Let's watch cat images emerge from noise! This is the reverse of what we saw in the forward process visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vis-reverse-codeZZ2U"
      },
      "outputs": [],
      "source": [
        "# Show the reverse diffusion trajectory for one sample\n",
        "fig, axes = plt.subplots(1, len(trajectory), figsize=(14, 2))\n",
        "timesteps_labels = ['t=999', 't=900', 't=750', 't=500', 't=250', 't=100', 't=50', 't=0']\n",
        "\n",
        "for ax, traj, label in zip(axes, trajectory, ['t=1000'] + timesteps_labels):\n",
        "    img = traj[0].permute(1, 2, 0)  # First sample\n",
        "    img = (img + 1) / 2  # [-1,1] to [0,1]\n",
        "    ax.imshow(img.clip(0, 1))\n",
        "    ax.set_title(label)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Reverse Diffusion: Noise \u2192 Cat', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "compareZZ2U"
      },
      "source": [
        "## Generated Samples\n",
        "\n",
        "Let's look at a grid of generated cat images and compare them to real cats from CIFAR-10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "compare-codeZZ2U"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 8, figsize=(12, 3))\n",
        "\n",
        "# Generated samples\n",
        "for i in range(8):\n",
        "    img = generated_images[i].cpu().permute(1, 2, 0)\n",
        "    img = (img + 1) / 2\n",
        "    axes[0, i].imshow(img.clip(0, 1))\n",
        "    axes[0, i].axis('off')\n",
        "    if i == 0:\n",
        "        axes[0, i].set_ylabel('Generated', fontsize=12)\n",
        "\n",
        "# Real samples for comparison\n",
        "for i in range(8):\n",
        "    img, _ = train_cats[i + 100]  # Different samples than shown earlier\n",
        "    img = img.permute(1, 2, 0)\n",
        "    img = (img + 1) / 2\n",
        "    axes[1, i].imshow(img.clip(0, 1))\n",
        "    axes[1, i].axis('off')\n",
        "    if i == 0:\n",
        "        axes[1, i].set_ylabel('Real', fontsize=12)\n",
        "\n",
        "plt.suptitle('Generated vs Real Cat Images', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Top row: Generated samples from our trained model\")\n",
        "print(\"Bottom row: Real cat images from CIFAR-10\")\n",
        "print(\"\\nNote: With more training epochs and a larger model, quality improves significantly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "takeawaysZZ2U"
      },
      "source": [
        "## Key Takeaways\n",
        "\n",
        "This notebook demonstrated DDPM applied to real images:\n",
        "\n",
        "**Same Mathematics**: The forward process, loss function, and sampling algorithm are identical to the toy example. Only the data and network architecture changed.\n",
        "\n",
        "**U-Net Architecture**: For images, we use a U-Net instead of an MLP. The encoder-decoder structure with skip connections is essential for preserving spatial details.\n",
        "\n",
        "**Scale Considerations**: CIFAR-10 is still a small dataset with low resolution. State-of-the-art models use:\n",
        "- Much larger U-Nets with attention mechanisms\n",
        "- Higher resolutions (256\u00d7256, 512\u00d7512, or more)\n",
        "- More training steps and larger batch sizes\n",
        "- Techniques like classifier-free guidance for better sample quality\n",
        "\n",
        "**From Toy to Real**: The jump from 2D points to 32\u00d732 images demonstrates that diffusion models scale naturally to higher dimensions. The same principles apply whether generating simple shapes or photorealistic images!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}